https://medium.com/upshothq/peer-prediction-101-ba267d8b79ee

### Can we build a classroom without a teacher?
We can take for granted an ample supply of eager students, communication tools for them to coordinate with each other, and plenty of engaging material online. There are also plenty of readily-available exercises and exams to quiz oneself, topics to write about, and real-world problems to solve. Many of these resources are also already pre-assembled into courses, so self-motivated students know exactly what they should study and in what cadence they should study it.

What remains, however, is crucially important: How do we grade students without a teacher? This is crucial because, otherwise, how are others supposed to gauge whether or not students actually absorbed the material?

The “teacherless classroom” (e.g. edX) is the canonical setting in which peer prediction is studied. In general, peer prediction asks “How do we grade answers without a rubric, while only using said answers?” In this post, after motivating why we might ask such a question in the first place, we introduce and briefly trace the history of this budding, overlooked academic field. In doing so, we’ll understand why the “teacherless classroom” is representative of a myriad of other settings (e.g. curation, verification, governance) and thus appreciate how and why peer prediction is widely applicable.

### Grading with a Teacher
Teachers provide an incentive to produce high-quality work. This is because teachers are sources of ground-truth — they look at our work, they query their knowledge-base to gauge what “correct” or “good” work is, and, on our work, they mark any discrepancies with red ink. As hinted, our work may be compared with objective facts (e.g. “You said 2+2 is 5, but it’s actually 4.”) or subjective beliefs (e.g. “This essay is coherent and well-written.”). Thus, teachers provide a ton of convenience in the sense that, within their jurisdiction, their truths are ground-truth. This affects the incentives governing students — they strive to submit answers, write essays, etc. that are as closely aligned with the given ground-truth as possible. Students partake in teacher-led education because they assume (usually justifiably) that a teacher’s ground-truth aligns with a more universal ground-truth.

Teachers make it easy to query the quality of our work, but there’s a catch — they must be trusted. We implicitly rely on professional certifications to ensure that teachers are deserving of this trust, but, unfortunately, there are plenty of settings in which the analogues of teachers cannot so easily be trusted. This has the important, undesirable effect of making the analogues of students either motivated in an undesirable way or complacent.

Consider a prediction market that guesses the outcome of an election. It eventually needs to query an oracle — the “teacher” querying their objective rubric — to learn what the outcome was. Clearly, traders on all sides of the prediction market have a vested interest in manipulating this oracle. Furthermore, the incentives governing traders may be altered — instead of betting on the outcome of the election, they may instead be betting on the output of an oracle that is independent of the election’s results. This undermines the desired predictive aspects of prediction markets.

Consider a poll where each participant is incentivized to answer subjective questions (e.g. What is your favorite fast-food chain?) via some reward. The pollster has no given subjective rubric with which to grade participants, so there is no way for them to distinguish between honest and dishonest responses. Hence, participants are free to randomly answer questions with arbitrary honesty, because their reward will be independent of their answers.

### Grading with Peer Prediction
Peer prediction endeavors to remedy the problem highlighted above; peer prediction mechanisms (or, schemes) incentivize honest, high-quality answers by examining the structure of answers themselves. For example, in the teacherless classroom, students grade each other, and the grades they give one another determine how skilled of a grader each student is. In the polling example, participants’ opinions are used to determine how honest each participant is.

Peer prediction mechanisms accomplish this by using measures of correlation that input answers to questions from participants and outputs scores for participants. These scores determine the quality of each agent’s answers. “In the wild,” these scores may be tied to financial incentives; participants may win or lose money based on the quality of their answers. With scores in hand, determining the “actual answer” to a question is straightforward — we can simply take the answers from the agent with the highest score (i.e. the agent with the highest quality answers).

For example, consider three students (A, B, C) who must grade a fourth student (D) on an assignment. Say A and B each give D a 100% except for student C, who gave student D an 80%. A simple correlation measure may be “You get a 1 if you agreed with the majority and 0 otherwise.” Hence, A and B would get a score of 1 and C would get a score of 0, and D would receive a 100%. Note how similar this scheme is to the well-known SchellingCoin game (many of us are already exposed to peer prediction without even knowing it!).

Peer prediction mechanisms are distinguished from one another by their choice of correlation measure. Hence, the history of peer prediction is a history of increasingly better measures of correlation. By understanding this history, we can understand why one may choose one such peer prediction mechanism over others. Furthermore, the history of peer prediction provides us with concrete examples of how peer prediction mechanisms operate.